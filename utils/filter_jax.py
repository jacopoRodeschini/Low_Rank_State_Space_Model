#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Thu Jun 19 10:16:39 2025

@author: jacopo
@title: Filter function with jax
"""

import jax
import jax.numpy as jnp
from jax.scipy.linalg import solve
from jax import jit

# %% KF - Filter smw nan


@jit
def filter_smw_nan(y_t, R, F, H, Q, x0, Sigma0, Xbeta, beta):
    """
    Optimized Kalman Filter using jax.lax.scan for variable-length inputs.
    """
    p, T = y_t.shape
    q = F.shape[0]

    # Pre-compute constants
    Iq = jnp.eye(q, dtype=jnp.float32)
    R_diag = R.diagonal()
    invR_diag = jnp.reciprocal(R_diag)
    invR = jnp.diag(invR_diag)
    H_dense = H.astype(jnp.float32)

    # This is the function for a single loop iteration
    def kalman_step(carry, step_data):
        # 1. Unpack carry and step_data
        x_prev, P_prev, logL_accum = carry
        yt_slice, Xbeta_slice = step_data

        # PREDICTION
        x_pred = F @ x_prev
        P_pred = F @ P_prev @ F.T + Q

        # RESIDUAL
        nan_mask = jnp.isnan(yt_slice)
        Xb = Xbeta_slice @ beta
        e = yt_slice - Xb - (H @ x_pred)
        e = jnp.where(nan_mask, 0.0, e)

        # MODIFIED H
        Hna_dense = H_dense * (~nan_mask)[:, None]

        # WOODBURY
        invP_pred = solve(P_pred, Iq)
        M = invP_pred + Hna_dense.T @ (invR @ Hna_dense)
        invM = solve(M, Iq)
        invSigmaE = invR - invR @ Hna_dense @ invM @ Hna_dense.T @ invR

        # KALMAN GAIN
        K = P_pred @ Hna_dense.T @ invSigmaE

        # UPDATE STATE
        x_upd = x_pred + K @ e
        P_upd = P_pred - K @ Hna_dense @ P_pred

        # LOG-LIKELIHOOD
        logdetSigmaE = (
            jnp.linalg.slogdet(M)[1]
            + jnp.linalg.slogdet(P_pred)[1]
            + jnp.sum(jnp.log(R_diag))
        )
        logL_accum += logdetSigmaE + e.T @ (invSigmaE @ e)

        # 2. Pack carry for next step and outputs for this step
        next_carry = (x_upd, P_upd, logL_accum)
        outputs = {
            "x_t": x_upd, "P_t": P_upd, "K": K,
            "x_t_1": x_pred, "P_t_1": P_pred, "invP_t_1": invP_pred
        }
        return next_carry, outputs

    # Prepare initial state and inputs for scan
    # The scan will iterate over the time dimension
    initial_carry = (x0, Sigma0, jnp.float32(0.0))
    # We need to transpose inputs so that T is the leading dimension
    # y_t: [p, T] -> [T, p]
    # Xbeta: [p, b, T] -> [T, p, b]
    scan_inputs = (y_t.T, jnp.moveaxis(Xbeta, -1, 0))

    # Run the scan
    (final_x, final_P, final_logL), history = jax.lax.scan(
        kalman_step, initial_carry, scan_inputs
    )

    # Post-process the results from the history dictionary
    # The outputs will have T as the leading dimension, so we move it back
    x_t = jnp.moveaxis(history["x_t"], 0, -1)
    P_t = jnp.moveaxis(history["P_t"], 0, -1)
    K = history["K"][-1]  # Often only the final gain is needed
    x_t_1 = jnp.moveaxis(history["x_t_1"], 0, -1)
    P_t_1 = jnp.moveaxis(history["P_t_1"], 0, -1)
    invP_t_1 = jnp.moveaxis(history["invP_t_1"], 0, -1)

    # Add the initial state to the beginning of the time series arrays
    x_t = jnp.concatenate([x0[:, None], x_t], axis=1)
    x_t_1 = jnp.concatenate([jnp.zeros((q, 1)), x_t_1], axis=1)

    P_t = jnp.concatenate([Sigma0[:, :, None], P_t], axis=2)
    P_t_1 = jnp.concatenate(
        [jnp.zeros(Sigma0.shape)[:, :, None], P_t_1], axis=2)
    invP_t_1 = jnp.concatenate(
        [jnp.diag(1/Sigma0.diagonal())[:, :, None], invP_t_1], axis=2)

    logL = jnp.float32(-0.5) * final_logL

    return x_t, P_t, K, x_t_1, P_t_1, invP_t_1, logL



# %% KF - Smoother SMW


@jit
def smoother_smw(H, F, x_t, P_t, Klast, x_t_1, P_t_1, invP_t_1):
    """
    Kalman smoother using jax.lax.scan for efficient, T-independent compilation.
    """
    p, q = H.shape
    T = x_t.shape[1] - 1

    def smoother_step(carry, inputs):
        """
        The body of the loop for a single time step.
        This function will be compiled once by `scan`.
        """
        # 1. Unpack the carry (state from the previous iteration, i.e., time t+1)
        x_T_next, P_T_next, P_T_1_next = carry

        # 2. Unpack the inputs for the current iteration (i.e., time t)
        # Note: The lag-one covariance calc needs P_t[t-2] and invP_t_1[t-1],
        # so we pass them in as well.
        x_t_curr, P_t_curr, x_t_1_next, P_t_1_next, invP_t_1_next, P_t_prev, invP_t_1_curr = inputs

        # --- Core Smoother Logic (from your original loop) ---
        J_t_1 = P_t_curr @ F.T @ invP_t_1_next

        x_T_curr = x_t_curr + J_t_1 @ (x_T_next - x_t_1_next)
        P_T_curr = P_t_curr + J_t_1 @ (P_T_next - P_t_1_next) @ J_t_1.T

        # Lag-one covariance
        J_t_2 = P_t_prev @ F.T @ invP_t_1_curr
        term = P_T_1_next - F @ P_t_curr
        P_T_1_curr = P_t_curr @ J_t_2.T + J_t_1 @ term @ J_t_2.T

        # 3. Prepare the new carry for the next iteration (time t-1)
        new_carry = (x_T_curr, P_T_curr, P_T_1_curr)

        # 4. Define what to store/stack at each iteration
        stacked_output = (x_T_curr, P_T_curr, P_T_1_curr)

        return new_carry, stacked_output

    # --- Prepare inputs for jax.lax.scan ---

    # 1. Initial Carry: The state at time T (the end of the data)
    # This is the starting point for the backward pass.
    x_T_last = x_t[:, -1]
    P_T_last = P_t[:, :, -1]
    # Lag-one cov at T is special
    P_T_1_last = (jnp.eye(q) - Klast @ H) @ F @ P_t[:, :, -2]

    init_carry = (x_T_last, P_T_last, P_T_1_last)

    # 2. Prepare the arrays to be scanned over (`xs`)
    # The loop runs from t=T-1 down to 0. We need to feed the scan function
    # the inputs in that order. We do this by stacking and reversing.

    # We need inputs from t=T-1, T-2, ..., 0
    # x_t_curr, P_t_curr, P_t_prev
    xs_x_t = x_t[:, :-1]
    xs_P_t = P_t[:, :, :-1]
    xs_P_t_prev = P_t[:, :, :-2]  # For lag-one cov

    # We need inputs from t=T, T-1, ..., 1
    # x_t_1_next, P_t_1_next, invP_t_1_next
    xs_x_t_1 = x_t_1[:, 1:]
    xs_P_t_1 = P_t_1[:, :, 1:]
    xs_invP_t_1 = invP_t_1[:, :, 1:]

    # We need inputs from t=T-1, T-2, ..., 0 for the lag-one cov's J_t_2
    xs_invP_t_1_curr = invP_t_1[:, :, 1:-1]

    # Pad the arrays that are too short to align them for stacking
    # We need T elements for the scan (from T-1 down to 0)
    # P_t_prev needs one pad, invP_t_1_curr needs one pad
    q_q_pad = jnp.zeros((q, q, 1))
    P_t_prev_padded = jnp.concatenate([q_q_pad, xs_P_t_prev], axis=2)
    invP_t_1_curr_padded = jnp.concatenate([q_q_pad, xs_invP_t_1_curr], axis=2)

    # Now, put them into a tuple and reverse for the backward pass
    # Transposing to (T, ...) shape for scan
    xs = (
        xs_x_t.T,
        jnp.moveaxis(xs_P_t, 2, 0),
        xs_x_t_1.T,
        jnp.moveaxis(xs_P_t_1, 2, 0),
        jnp.moveaxis(xs_invP_t_1, 2, 0),
        jnp.moveaxis(P_t_prev_padded, 2, 0),
        jnp.moveaxis(invP_t_1_curr_padded, 2, 0)
    )
    # Reverse time axis for backward pass
    xs_reversed = jax.tree.map(lambda x: jnp.flip(x, axis=0), xs)

    # --- Run the scan ---
    # The final carry is not needed, we use the stacked outputs
    _, (x_T_scanned, P_T_scanned, P_T_1_scanned) = jax.lax.scan(
        smoother_step, init_carry, xs_reversed
    )

    # --- Post-process the results ---
    # The outputs are also in reverse time order, so flip them back
    x_T_scanned = jnp.flip(x_T_scanned.T, axis=1)
    P_T_scanned = jnp.flip(jnp.moveaxis(P_T_scanned, 0, 2), axis=2)
    P_T_1_scanned = jnp.flip(jnp.moveaxis(P_T_1_scanned, 0, 2), axis=2)

    # Combine the scanned results (for t=0 to T-1) with the initial values (at t=T)
    x_T = jnp.concatenate([x_T_scanned, x_T_last[:, None]], axis=1)
    P_T = jnp.concatenate([P_T_scanned, P_T_last[:, :, None]], axis=2)

    # For P_T_1, the last element is special
    P_T_1 = jnp.concatenate([P_T_1_scanned, P_T_1_last[:, :, None]], axis=2)

    return x_T, P_T, P_T_1

# %% KF - Expected values

@jit
def computeExpectedValues(y_t, x_T, P_T, P_T_1, R, H, Xbeta, beta):
 
    # Note: Type conversions are omitted for clarity. It's often better
    # to ensure inputs have the correct dtype before calling a JIT-compiled function.

    # Slices of the smoothed states
    # x_t terms range from t=1 to T
    # x_{t-1} terms range from t=0 to T-1
    x_t_slice = x_T[:, 1:]      # Shape: [q, T]
    x_tm1_slice = x_T[:, :-1]   # Shape: [q, T]

    # --- 1. Compute predicted observations (y_hat) ---
    # The term Xbeta @ beta can be computed efficiently using einsum.
    # y_hat_t = Xbeta_t @ beta + H @ x_t
    y_hat_covariate_term = jnp.einsum('pkt,k->pt', Xbeta, beta)
    y_hat_state_term = H @ x_t_slice
    y_hat = y_hat_covariate_term + y_hat_state_term

    # --- 2. Compute sufficient statistics (S11, S10, S00) ---
    # E[sum(x x')] = sum(E[x]E[x]' + Cov(x)) = sum(x_T x_T') + sum(P_T)
    # The sum of outer products (x @ x') can be vectorized as X @ X.T

    # S11 = E[sum_{t=1..T} x_t x_t']
    # We need sums over t=1 to T
    S11 = (x_t_slice @ x_t_slice.T) + jnp.sum(P_T[:, :, 1:], axis=2)

    # S00 = E[sum_{t=1..T} x_{t-1} x_{t-1}']
    # We need sums over t-1=0 to T-1
    S00 = (x_tm1_slice @ x_tm1_slice.T) + jnp.sum(P_T[:, :, :-1], axis=2)

    # S10 = E[sum_{t=1..T} x_t x_{t-1}']
    # P_T_1 is Cov(x_t, x_{t-1}), so the sum starts from t=1
    S10 = (x_t_slice @ x_tm1_slice.T) + jnp.sum(P_T_1[:, :, 1:], axis=2)

    return y_hat, S11, S10, S00




